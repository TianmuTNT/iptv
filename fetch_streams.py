import requests
import pandas as pd
import re
import os
import time
from concurrent.futures import ThreadPoolExecutor

# é…ç½®å‚æ•°
MAX_SOURCES_PER_CHANNEL = 10  # æ¯ä¸ªé¢‘é“ä¿ç•™æºæ•°
REQUEST_TIMEOUT = 5  # æºè´¨é‡æ£€æµ‹è¶…æ—¶(ç§’)
THREAD_POOL = 10  # å¹¶å‘æ£€æµ‹çº¿ç¨‹æ•°

# ç›´æ’­æºURLåˆ—è¡¨
urls = [
    "http://8.138.7.223/live.txt",
    "https://7337.kstore.space/twkj/tvzb.txt",
    "https://ghfast.top/https://raw.githubusercontent.com/tianya7981/jiekou/refs/heads/main/%E9%87%8E%E7%81%AB959",
    "http://tot.totalh.net/tttt.txt",
    "https://raw.githubusercontent.com/YanG-1989/m3u/main/Gather.m3u",
    "https://raw.githubusercontent.com/YueChan/Live/refs/heads/main/APTV.m3u",
    "https://raw.githubusercontent.com/Kimentanm/aptv/master/m3u/iptv.m3u",
    'https://raw.githubusercontent.com/BurningC4/Chinese-IPTV/master/TV-IPV4.m3u',
    "https://raw.githubusercontent.com/Ftindy/IPTV-URL/main/IPV6.m3u",
]

# é¢‘é“è¿‡æ»¤æ­£åˆ™
channel_pattern = re.compile(
    r'CCTV|å«è§†',
    re.IGNORECASE
)

# åè®®è¯†åˆ«æ­£åˆ™
ipv4_pattern = re.compile(r'^https?://(\d{1,3}\.){3}\d{1,3}')
ipv6_pattern = re.compile(r'^https?://\[([a-fA-F0-9:]+)\]')

def fetch_streams_from_url(url):
    print(f"ğŸ“¡ æ­£åœ¨æŠ“å–æº: {url}")
    try:
        response = requests.get(url, timeout=10)
        response.encoding = 'utf-8'
        return response.text if response.status_code == 200 else None
    except Exception as e:
        print(f"âŒ æŠ“å–å¤±è´¥ {url}: {str(e)}")
        return None

def fetch_all_streams():
    with ThreadPoolExecutor(max_workers=5) as executor:
        results = executor.map(fetch_streams_from_url, urls)
    return "\n".join(filter(None, results))

def parse_m3u(content):
    streams = []
    current = {}
    for line in content.splitlines():
        if line.startswith("#EXTINF"):
            current["meta"] = line
            current["name"] = re.search(r'tvg-name="([^"]+)"', line).group(1)
        elif line.startswith("http"):
            streams.append({
                "program_name": current.get("name", "æœªçŸ¥é¢‘é“"),
                "stream_url": line.strip(),
                "meta": current.get("meta", "")
            })
    return streams

def parse_txt(content):
    streams = []
    for line in content.splitlines():
        if match := re.match(r"(.+?),(https?://.+)", line):
            streams.append({
                "program_name": match.group(1).strip(),
                "stream_url": match.group(2).strip(),
                "meta": ""
            })
    return streams

def check_source_quality(url):
    """æ£€æµ‹æºå“åº”é€Ÿåº¦å’Œè´¨é‡"""
    try:
        start = time.time()
        with requests.get(url, timeout=REQUEST_TIMEOUT, stream=True) as r:
            if r.status_code == 200:
                speed = time.time() - start
                return {"url": url, "speed": speed, "valid": True}
    except:
        pass
    return {"url": url, "speed": 999, "valid": False}

def filter_sources(sources):
    """è¿‡æ»¤å¹¶æ’åºæº"""
    with ThreadPoolExecutor(THREAD_POOL) as executor:
        results = list(executor.map(check_source_quality, sources))
    
    valid_sources = sorted(
        [r for r in results if r["valid"]],
        key=lambda x: x["speed"]
    )
    return [s["url"] for s in valid_sources[:MAX_SOURCES_PER_CHANNEL]]

def organize_streams(content):
    # è§£æå†…å®¹
    parser = parse_m3u if content.startswith("#EXTM3U") else parse_txt
    streams = parser(content)
    
    # è½¬æ¢ä¸ºDataFrame
    df = pd.DataFrame(streams)
    
    if df.empty:
        return pd.DataFrame()
    
    # è¿‡æ»¤é¢‘é“
    df = df[df["program_name"].str.contains(channel_pattern, na=False)]
    
    # åˆ†ç»„å¤„ç† (ä¿®å¤ä¸¢å¤± program_name é—®é¢˜)
    grouped = (
        df
        .groupby("program_name", group_keys=False)
        # æ˜¾å¼é€‰æ‹©éœ€è¦æ“ä½œçš„åˆ— (æ’é™¤åˆ†ç»„é”®)
        [["stream_url", "meta"]]  
        .apply(lambda x: (
            x
            .drop_duplicates(subset="stream_url")
            .head(100)
        ))
        # é‡å»ºå®Œæ•´æ•°æ®ç»“æ„
        .reset_index()  
    )
    
    # åˆ†ç»„ç­›é€‰æœ€ä½³æº
    filtered = []
    for name, group in grouped.groupby("program_name"):
        print(f"ğŸ” æ­£åœ¨æ£€æµ‹é¢‘é“: {name}")
        best_sources = filter_sources(group["stream_url"].tolist())
        filtered.extend([
            {"program_name": name, "stream_url": url, "meta": group.iloc[0]["meta"]}
            for url in best_sources
        ])
    
    return pd.DataFrame(filtered)


def save_m3u(dataframe, filename="live.m3u"):
    filepath = os.path.abspath(filename)
    print(f"ğŸ’¾ æ­£åœ¨ä¿å­˜æ–‡ä»¶åˆ°: {filepath}")
    
    with open(filepath, "w", encoding="utf-8") as f:
        f.write("#EXTM3U x-tvg-url=\"\"\n")
        
        for _, row in dataframe.iterrows():
            # ç”Ÿæˆåˆ†ç»„ä¿¡æ¯
            protocol = "IPv6" if ipv6_pattern.match(row["stream_url"]) else "IPv4"
            
            # å†™å…¥é¢‘é“ä¿¡æ¯
            extinf = row["meta"] or f'#EXTINF:-1 tvg-name="{row["program_name"]}" group-title="{protocol}",{row["program_name"]}'
            f.write(f"{extinf}\n{row["stream_url"]}\n")
    
    print(f"âœ… ä¿å­˜å®Œæˆï¼æœ‰æ•ˆé¢‘é“æ•°ï¼š{dataframe['program_name'].nunique()}")

if __name__ == "__main__":
    print("ğŸš€ å¼€å§‹æŠ“å–ç›´æ’­æº...")
    content = fetch_all_streams()
    
    if content:
        print("ğŸ”„ æ­£åœ¨æ•´ç†é¢‘é“...")
        organized = organize_streams(content)
        
        if not organized.empty:
            print("ğŸ‰ æœ‰æ•ˆæºæ•´ç†å®Œæˆ")
            save_m3u(organized)
        else:
            print("âš ï¸ æœªæ‰¾åˆ°ç¬¦åˆè¦æ±‚çš„é¢‘é“")
    else:
        print("âŒ æœªèƒ½è·å–æœ‰æ•ˆå†…å®¹")
